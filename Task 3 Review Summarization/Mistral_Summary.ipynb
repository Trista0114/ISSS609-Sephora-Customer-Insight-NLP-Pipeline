{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwCupX3DvSPr",
    "outputId": "2f7d705e-7619-4878-98a2-b0e96a54c8d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.48.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 830,
     "referenced_widgets": [
      "d74d8458627348fab266cbafe5994c81",
      "2465ac4619cf41f29f5a94b73b6b9d82",
      "d7760ccf35254ba7a8afe8ee6c642cbf",
      "2006d23350fb4430abad9917a1d66502",
      "a48cae334a8d4118acf342aef0f6c62e",
      "4eeefdca8de74f16b9f7695ed33f3b11",
      "3ed654816494488e8fa34b9a805e29bc",
      "7ff2afc743404086a5c6eadd95e5461d",
      "034bb2c9eaed424db500509e70c67e7f",
      "e7b4167876b04d42adff9b7957e008e5",
      "111951361eae4946a780a78ea882ecbe"
     ]
    },
    "id": "T0oTHeS7ztC8",
    "outputId": "731801bf-4151-4927-ae5b-ddbbfe402625"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74d8458627348fab266cbafe5994c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed row 1/42\n",
      "✅ Processed row 2/42\n",
      "✅ Processed row 3/42\n",
      "✅ Processed row 4/42\n",
      "✅ Processed row 5/42\n",
      "✅ Processed row 6/42\n",
      "✅ Processed row 7/42\n",
      "✅ Processed row 8/42\n",
      "✅ Processed row 9/42\n",
      "✅ Processed row 10/42\n",
      "✅ Processed row 11/42\n",
      "✅ Processed row 12/42\n",
      "✅ Processed row 13/42\n",
      "✅ Processed row 14/42\n",
      "✅ Processed row 15/42\n",
      "✅ Processed row 16/42\n",
      "✅ Processed row 17/42\n",
      "✅ Processed row 18/42\n",
      "✅ Processed row 19/42\n",
      "✅ Processed row 20/42\n",
      "✅ Processed row 21/42\n",
      "✅ Processed row 22/42\n",
      "✅ Processed row 23/42\n",
      "✅ Processed row 24/42\n",
      "✅ Processed row 25/42\n",
      "✅ Processed row 26/42\n",
      "✅ Processed row 27/42\n",
      "✅ Processed row 28/42\n",
      "✅ Processed row 29/42\n",
      "✅ Processed row 30/42\n",
      "✅ Processed row 31/42\n",
      "✅ Processed row 32/42\n",
      "✅ Processed row 33/42\n",
      "✅ Processed row 34/42\n",
      "✅ Processed row 35/42\n",
      "✅ Processed row 36/42\n",
      "✅ Processed row 37/42\n",
      "✅ Processed row 38/42\n",
      "✅ Processed row 39/42\n",
      "✅ Processed row 40/42\n",
      "✅ Processed row 41/42\n",
      "✅ Processed row 42/42\n",
      "\n",
      "✅ Saved clean summaries only to: task3_Review_summarisation_mistral_both_clean_v2.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ==========================\n",
    "# 0) Configuration\n",
    "# ==========================\n",
    "INPUT_XLSX = \"Task3_Review_Summarization_Generative_RuleBased_v2.xlsx\"\n",
    "OUTPUT_CSV  = \"task3_Review_summarisation_mistral_both_clean_v2.csv\"\n",
    "PROS_COL = \"Pros Summary\"\n",
    "CONS_COL = \"Cons Summary\"\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# ==========================\n",
    "# 1) Load Model\n",
    "# ==========================\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "\n",
    "# ==========================\n",
    "# 2) Helper Functions\n",
    "# ==========================\n",
    "def _clean_text(x):\n",
    "    \"\"\"Ensure clean text input (no NaNs or floats).\"\"\"\n",
    "    if isinstance(x, float) and pd.isna(x):\n",
    "        return \"\"\n",
    "    return x if isinstance(x, str) else \"\"\n",
    "\n",
    "def _generate(prompt, max_new_tokens=320, temperature=0.3):\n",
    "    \"\"\"Generate only the model continuation (exclude echoed prompt).\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    prompt_len = inputs.input_ids.shape[1]  # number of tokens in the prompt\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=0.9,\n",
    "        do_sample=False,\n",
    "        pad_token_id=pad_id\n",
    "    )\n",
    "\n",
    "    # Keep only the newly generated tokens after the prompt\n",
    "    new_tokens = out[0][prompt_len:]\n",
    "    text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    return text.strip()\n",
    "\n",
    "# ==========================\n",
    "# 3) Post-Cleaners (remove echoed prompts)\n",
    "# ==========================\n",
    "def clean_generated_text(text):\n",
    "    \"\"\"\n",
    "    Remove any leftover prompt text from model output.\n",
    "    Works for both neutral and persuasive outputs.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Remove common echoed parts like \"PROS:\", \"CONS:\", \"Use only this input:\", etc.\n",
    "    text = re.sub(r\"(?is)(Use only this input:|PROS:|CONS:|Positive opinions:|Negative opinions:|Positive aspects:|Negative aspects:).*?(?=\\n|$)\", \"\", text)\n",
    "\n",
    "    # Remove multiple blank lines\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "\n",
    "    # Trim excessive leading/trailing spaces and lines\n",
    "    text = text.strip()\n",
    "\n",
    "    # Remove any residual instructional sentences\n",
    "    text = re.sub(r\"(?i)(You are a careful copywriter|Write the summary now|Style & rules:).*\", \"\", text).strip()\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# ==========================\n",
    "# 4) Neutral (Internal Team)\n",
    "# ==========================\n",
    "def summarise_with_mistral_neutral(pros, cons, max_new_tokens=320):\n",
    "    pros = _clean_text(pros)\n",
    "    cons = _clean_text(cons)\n",
    "    if not pros.strip() and not cons.strip():\n",
    "        return \"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a neutral summarization assistant.\n",
    "Summarize verified customer opinions, strictly using the provided text.\n",
    "\n",
    "Rules:\n",
    "- Neutral, factual tone.\n",
    "- No first-person.\n",
    "- Two sections with these exact headings: \"Positive aspects\" and \"Negative aspects\".\n",
    "- Reflect mixed opinions if present.\n",
    "- Do not add information that is not present in the input.\n",
    "\n",
    "---\n",
    "Positive opinions:\n",
    "{pros}\n",
    "\n",
    "Negative opinions:\n",
    "{cons}\n",
    "\n",
    "---\n",
    "Write the summary now.\n",
    "\"\"\"\n",
    "    output = _generate(prompt, max_new_tokens=max_new_tokens, temperature=0.3)\n",
    "    return clean_generated_text(output)\n",
    "\n",
    "# ==========================\n",
    "# 5) Persuasive (Customer-Facing)\n",
    "# ==========================\n",
    "def summarise_with_mistral_persuasive_paragraph(pros, cons, max_new_tokens=300):\n",
    "    pros = _clean_text(pros)\n",
    "    cons = _clean_text(cons)\n",
    "    if not pros.strip() and not cons.strip():\n",
    "        return \"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a careful copywriter. Write a concise product summary as smooth paragraphs, grounded ONLY in the provided opinions.\n",
    "\n",
    "Style & rules:\n",
    "- Paragraph prose only. No headings, no bullets.\n",
    "- Emphasize benefits from PROS (this is the main focus).\n",
    "- Mention CONS briefly in ONE short sentence with hedging (\"may\", \"for some users\"), only if CONS exist.\n",
    "- Neutral-to-positive tone, sales-friendly but truthful. No first-person. No invented claims.\n",
    "- Keep it tight: about 3–6 sentences total.\n",
    "\n",
    "Use only this input:\n",
    "PROS:\n",
    "{pros}\n",
    "\n",
    "CONS:\n",
    "{cons}\n",
    "\n",
    "Now write the summary.\n",
    "\"\"\"\n",
    "    output = _generate(prompt, max_new_tokens=max_new_tokens, temperature=0.35)\n",
    "    return clean_generated_text(output)\n",
    "\n",
    "# ==========================\n",
    "# 6) Generate Summaries\n",
    "# ==========================\n",
    "df = pd.read_excel(INPUT_XLSX)\n",
    "\n",
    "neutral_summaries = []\n",
    "persuasive_summaries = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    pros = row.get(PROS_COL, \"\")\n",
    "    cons = row.get(CONS_COL, \"\")\n",
    "\n",
    "    neutral = summarise_with_mistral_neutral(pros, cons)\n",
    "    persuasive = summarise_with_mistral_persuasive_paragraph(pros, cons)\n",
    "\n",
    "    neutral_summaries.append(neutral)\n",
    "    persuasive_summaries.append(persuasive)\n",
    "    print(f\"✅ Processed row {idx+1}/{len(df)}\")\n",
    "\n",
    "df[\"Overall_Summary_Mistral_Neutral\"] = neutral_summaries\n",
    "df[\"Overall_Summary_Mistral_Persuasive\"] = persuasive_summaries\n",
    "\n",
    "# ==========================\n",
    "# 7) Save Clean Output\n",
    "# ==========================\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\n✅ Saved clean summaries only to: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwqoGpeu1afu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "034bb2c9eaed424db500509e70c67e7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "111951361eae4946a780a78ea882ecbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2006d23350fb4430abad9917a1d66502": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7b4167876b04d42adff9b7957e008e5",
      "placeholder": "​",
      "style": "IPY_MODEL_111951361eae4946a780a78ea882ecbe",
      "value": " 3/3 [00:04&lt;00:00,  1.44s/it]"
     }
    },
    "2465ac4619cf41f29f5a94b73b6b9d82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4eeefdca8de74f16b9f7695ed33f3b11",
      "placeholder": "​",
      "style": "IPY_MODEL_3ed654816494488e8fa34b9a805e29bc",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "3ed654816494488e8fa34b9a805e29bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4eeefdca8de74f16b9f7695ed33f3b11": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ff2afc743404086a5c6eadd95e5461d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a48cae334a8d4118acf342aef0f6c62e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d74d8458627348fab266cbafe5994c81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2465ac4619cf41f29f5a94b73b6b9d82",
       "IPY_MODEL_d7760ccf35254ba7a8afe8ee6c642cbf",
       "IPY_MODEL_2006d23350fb4430abad9917a1d66502"
      ],
      "layout": "IPY_MODEL_a48cae334a8d4118acf342aef0f6c62e"
     }
    },
    "d7760ccf35254ba7a8afe8ee6c642cbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ff2afc743404086a5c6eadd95e5461d",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_034bb2c9eaed424db500509e70c67e7f",
      "value": 3
     }
    },
    "e7b4167876b04d42adff9b7957e008e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
