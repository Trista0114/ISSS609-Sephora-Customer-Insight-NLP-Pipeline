{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a5b9d7d-8869-469c-8e84-e340c5bc250b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product-level columns: ['product_name_cleaned', 'Theme_id', 'Theme_name', 'subtopic_id', 'subtopic_name', 'FreqWeight (Topic Prominence)', 'Subtopic_AvgSentiment', 'SubTopicScore', 'ProductAvgSentiment', 'RelativeSubtopicSentiment']\n",
      "Sentence-level columns: ['Unnamed: 0.1', 'Unnamed: 0', 'author_id', 'rating', 'is_recommended', 'helpfulness', 'total_feedback_count', 'total_neg_feedback_count', 'total_pos_feedback_count', 'submission_time', 'review_text', 'review_title', 'skin_tone', 'eye_color', 'skin_type', 'hair_color', 'product_id', 'product_name', 'brand_name', 'price_usd', 'primary_category', 'secondary_category', 'tertiary_category', 'variation_type', 'variation_value', 'variation_desc', 'review_seq_id', 'product_name_cleaned', 'review_count', 'rank_in_brand', '_informative_', 'author_id_f', 'sentence_text', 'sentence_index', 'n_sentences_in_review', 'sentence_char_len', 'clean_sentence_lda', 'global_topic_id', 'global_topic_name', 'final_subtopic_id', 'final_subtopic_name', 'max_subtopic_prob', 'subtopic_prob_101', 'subtopic_prob_100', 'subtopic_prob_400', 'subtopic_prob_401', 'subtopic_prob_402', 'subtopic_prob_403', 'subtopic_prob_500', 'subtopic_prob_501', 'subtopic_prob_502', 'subtopic_prob_503', 'subtopic_prob_504', 'subtopic_prob_600', 'subtopic_prob_601', 'subtopic_prob_602', 'cosine_to_centroid', 'sentiment_score']\n",
      "ID-merge coverage: 100.0%\n",
      "\n",
      "Label distribution:\n",
      "Category\n",
      "Pro    30355\n",
      "Con    11210\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample output:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name_cleaned</th>\n",
       "      <th>Top_ProsCons_Topics</th>\n",
       "      <th>Pros</th>\n",
       "      <th>Cons</th>\n",
       "      <th>Pros Summary</th>\n",
       "      <th>Cons Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A-Passioni Retinol Cream</td>\n",
       "      <td>300, 100, 400, 500, 504, 402, 403, 200, 601, 602</td>\n",
       "      <td>[Definitely the most effective skincare produc...</td>\n",
       "      <td>[It was very disappointing given the high pric...</td>\n",
       "      <td>Definitely the most effective skincare product...</td>\n",
       "      <td>It was very disappointing given the high price...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AHA 30% + BHA 2% Exfoliating Peeling Solution</td>\n",
       "      <td>300, 402, 401, 100, 200, 601, 602, 600, 501, 500</td>\n",
       "      <td>[I can't wait to see how much more my skin imp...</td>\n",
       "      <td>[extraordinarily terrible for my skin i am ble...</td>\n",
       "      <td>I can't wait to see how much more my skin impr...</td>\n",
       "      <td>extraordinarily terrible for my skin i am blea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acne Solutions Cleansing Foam</td>\n",
       "      <td>300, 200, 504, 101, 600, 402, 401, 403, 601, 400</td>\n",
       "      <td>[It is the best for get my face clean and keep...</td>\n",
       "      <td>[Cleanses fine but very unpleasant smell., The...</td>\n",
       "      <td>It is the best for get my face clean and keepi...</td>\n",
       "      <td>Cleanses fine but very unpleasant smell. There...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All About Clean Liquid Facial Soap</td>\n",
       "      <td>200, 300, 504, 600, 503, 401, 403, 400, 402, 100</td>\n",
       "      <td>[This stuff feels so great when you wash your ...</td>\n",
       "      <td>[This dries my skin out so badly that I have m...</td>\n",
       "      <td>This stuff feels so great when you wash your f...</td>\n",
       "      <td>This dries my skin out so badly that I have mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alpha Beta Extra Strength Daily Peel Pads</td>\n",
       "      <td>100, 300, 101, 504, 503, 403, 400, 402, 401, 200</td>\n",
       "      <td>[Bright and fresh face without leaving my skin...</td>\n",
       "      <td>[A lot of money to pay for nothing., I hate lo...</td>\n",
       "      <td>Bright and fresh face without leaving my skin ...</td>\n",
       "      <td>A lot of money to pay for nothing. I hate losi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            product_name_cleaned  \\\n",
       "0                       A-Passioni Retinol Cream   \n",
       "1  AHA 30% + BHA 2% Exfoliating Peeling Solution   \n",
       "2                  Acne Solutions Cleansing Foam   \n",
       "3             All About Clean Liquid Facial Soap   \n",
       "4      Alpha Beta Extra Strength Daily Peel Pads   \n",
       "\n",
       "                                Top_ProsCons_Topics  \\\n",
       "0  300, 100, 400, 500, 504, 402, 403, 200, 601, 602   \n",
       "1  300, 402, 401, 100, 200, 601, 602, 600, 501, 500   \n",
       "2  300, 200, 504, 101, 600, 402, 401, 403, 601, 400   \n",
       "3  200, 300, 504, 600, 503, 401, 403, 400, 402, 100   \n",
       "4  100, 300, 101, 504, 503, 403, 400, 402, 401, 200   \n",
       "\n",
       "                                                Pros  \\\n",
       "0  [Definitely the most effective skincare produc...   \n",
       "1  [I can't wait to see how much more my skin imp...   \n",
       "2  [It is the best for get my face clean and keep...   \n",
       "3  [This stuff feels so great when you wash your ...   \n",
       "4  [Bright and fresh face without leaving my skin...   \n",
       "\n",
       "                                                Cons  \\\n",
       "0  [It was very disappointing given the high pric...   \n",
       "1  [extraordinarily terrible for my skin i am ble...   \n",
       "2  [Cleanses fine but very unpleasant smell., The...   \n",
       "3  [This dries my skin out so badly that I have m...   \n",
       "4  [A lot of money to pay for nothing., I hate lo...   \n",
       "\n",
       "                                        Pros Summary  \\\n",
       "0  Definitely the most effective skincare product...   \n",
       "1  I can't wait to see how much more my skin impr...   \n",
       "2  It is the best for get my face clean and keepi...   \n",
       "3  This stuff feels so great when you wash your f...   \n",
       "4  Bright and fresh face without leaving my skin ...   \n",
       "\n",
       "                                        Cons Summary  \n",
       "0  It was very disappointing given the high price...  \n",
       "1  extraordinarily terrible for my skin i am blea...  \n",
       "2  Cleanses fine but very unpleasant smell. There...  \n",
       "3  This dries my skin out so badly that I have mo...  \n",
       "4  A lot of money to pay for nothing. I hate losi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities\n",
    "# ---------------------------\n",
    "def clean_text(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    replacements = {\n",
    "        \"â€™\": \"'\",\n",
    "        \"â€œ\": '\"',\n",
    "        \"â€\": '\"',\n",
    "        \"â€“\": \"–\",\n",
    "        \"â€”\": \"–\",\n",
    "        \"â€¦\": \"...\",\n",
    "        \"Â\": \"\"\n",
    "    }\n",
    "    for bad, good in replacements.items():\n",
    "        text = text.replace(bad, good)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def normalize_name(s):\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = re.sub(r\"[^\\w\\s\\-&/]+$\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def safe_sheet_name(name, max_len=31):\n",
    "    return name[:max_len]\n",
    "\n",
    "def infer_sentiment_column(df):\n",
    "    \"\"\"\n",
    "    Try to find a usable sentiment column.\n",
    "    Preference order:\n",
    "      - columns containing 'sentiment' or 'compound'\n",
    "    Returns (colname, scale) where scale is one of {\"pm1\",\"01\",\"15\"} meaning:\n",
    "      pm1: values roughly in [-1,1]\n",
    "      01 : values in [0,1]\n",
    "      15 : values in [1,5] (ratings)\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    for c in df.columns:\n",
    "        lc = c.lower()\n",
    "        if \"sent\" in lc or \"compound\" in lc or lc in {\"sentiment\", \"compound\"}:\n",
    "            if pd.api.types.is_numeric_dtype(df[c]):\n",
    "                nonnull = df[c].notna().mean()\n",
    "                if nonnull > 0.5:  # at least half filled\n",
    "                    candidates.append(c)\n",
    "    if not candidates:\n",
    "        raise ValueError(\"No numeric sentiment-like column found in the sentence-level table.\")\n",
    "\n",
    "    # pick the one with largest non-null coverage\n",
    "    best = max(candidates, key=lambda x: df[x].notna().mean())\n",
    "    s = df[best].dropna()\n",
    "    vmin, vmax = s.min(), s.max()\n",
    "\n",
    "    if vmin < 0 and vmax > 0:\n",
    "        scale = \"pm1\"           # typical VADER/transformer polarity\n",
    "    elif 0 <= vmin and vmax <= 1.00001:\n",
    "        scale = \"01\"            # probabilities\n",
    "    elif 1 <= vmin and vmax <= 5.00001:\n",
    "        scale = \"15\"            # star ratings\n",
    "    else:\n",
    "        # fallback: assume centered at 0 if it crosses 0; else treat as 0..1-like\n",
    "        scale = \"pm1\" if (vmin < 0 and vmax > 0) else \"01\"\n",
    "    print(f\"Using sentiment column: '{best}' (min={vmin:.3f}, max={vmax:.3f}, scale={scale})\")\n",
    "    return best, scale\n",
    "\n",
    "def label_sentiment_series(s, scale):\n",
    "    \"\"\"\n",
    "    Convert a numeric sentiment series to 'Pro' / 'Con' / 'Neutral' based on scale.\n",
    "    Your requirement: Pros = positive only, Cons = negative only.\n",
    "    \"\"\"\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    if scale == \"pm1\":   # [-1, 1]\n",
    "        pro = s > 0\n",
    "        con = s < 0\n",
    "    elif scale == \"01\":  # [0, 1]\n",
    "        pro = s >= 0.55\n",
    "        con = s <= 0.45\n",
    "    elif scale == \"15\":  # [1, 5]\n",
    "        pro = s >= 4.0\n",
    "        con = s <= 2.0\n",
    "    else:\n",
    "        pro = s > 0\n",
    "        con = s < 0\n",
    "    out = pd.Series(\"Neutral\", index=s.index)\n",
    "    out[pro] = \"Pro\"\n",
    "    out[con] = \"Con\"\n",
    "    return out\n",
    "\n",
    "# ---------------------------\n",
    "# Load data\n",
    "# ---------------------------\n",
    "product_df = pd.read_excel(\"Topic_sentiment_product_level_table_FINAL.xlsx\",\n",
    "                           sheet_name=\"Topic sentiment product level\")\n",
    "sentence_df = pd.read_excel(\"Topic_sentiment_sentence_level_table.xlsx\",\n",
    "                            sheet_name=\"Topic sentiment sentence level\")\n",
    "\n",
    "print(\"Product-level columns:\", product_df.columns.tolist())\n",
    "print(\"Sentence-level columns:\", sentence_df.columns.tolist())\n",
    "\n",
    "# Normalize product names\n",
    "product_df[\"product_name_cleaned_norm\"] = product_df[\"product_name_cleaned\"].apply(normalize_name)\n",
    "sentence_df[\"product_name_cleaned_norm\"] = sentence_df[\"product_name_cleaned\"].apply(normalize_name)\n",
    "\n",
    "# Build string IDs for joins\n",
    "product_df[\"Theme_id_str\"] = product_df[\"Theme_id\"].astype(str)\n",
    "sentence_df[\"final_subtopic_id_str\"] = sentence_df[\"final_subtopic_id\"].astype(str)\n",
    "\n",
    "# Normalize topic names for name-based fallback\n",
    "product_df[\"Theme_name_norm\"] = product_df[\"Theme_name\"].apply(normalize_name)\n",
    "sentence_df[\"final_subtopic_name_norm\"] = sentence_df[\"final_subtopic_name\"].apply(normalize_name)\n",
    "\n",
    "# ---------------------------\n",
    "# Topic ranking (composite)\n",
    "# ---------------------------\n",
    "prom_col = \"FreqWeight (Topic Prominence)\"\n",
    "sent_col_prod = \"Subtopic_AvgSentiment\"\n",
    "\n",
    "prod_rank = product_df.copy()\n",
    "prod_rank[\"Topic_Score\"] = 0.5 * prod_rank[sent_col_prod] + 0.5 * prod_rank[prom_col]\n",
    "prod_rank = prod_rank.sort_values([\"product_name_cleaned\", \"Topic_Score\"], ascending=[True, False])\n",
    "\n",
    "# Keep necessary columns\n",
    "prod_rank_min = prod_rank[[\n",
    "    \"product_name_cleaned\", \"subtopic_id\", \"subtopic_name\",\n",
    "    prom_col, sent_col_prod, \"Topic_Score\"\n",
    "]].rename(columns={\n",
    "    \"subtopic_id\": \"subtopic_id_str\",\n",
    "    \"subtopic_name\": \"subtopic_name_norm\"\n",
    "})\n",
    "\n",
    "# Convert types for matching\n",
    "prod_rank_min[\"subtopic_id_str\"] = prod_rank_min[\"subtopic_id_str\"].astype(str)\n",
    "sentence_df[\"final_subtopic_id_str\"] = sentence_df[\"final_subtopic_id\"].astype(str)\n",
    "\n",
    "# Normalize topic names\n",
    "prod_rank_min[\"subtopic_name_norm\"] = prod_rank_min[\"subtopic_name_norm\"].apply(normalize_name)\n",
    "sentence_df[\"final_subtopic_name_norm\"] = sentence_df[\"final_subtopic_name\"].apply(normalize_name)\n",
    "\n",
    "# ---------------------------\n",
    "# Merge using subtopic_id first\n",
    "# ---------------------------\n",
    "merged = sentence_df.merge(\n",
    "    prod_rank_min[[\"product_name_cleaned\", \"subtopic_id_str\", \"Topic_Score\"]],\n",
    "    left_on=[\"product_name_cleaned\", \"final_subtopic_id_str\"],\n",
    "    right_on=[\"product_name_cleaned\", \"subtopic_id_str\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "id_cov = merged[\"Topic_Score\"].notna().mean()\n",
    "print(f\"ID-merge coverage: {id_cov*100:.1f}%\")\n",
    "\n",
    "# ---------------------------\n",
    "# Fallback: merge by subtopic_name\n",
    "# ---------------------------\n",
    "if id_cov < 0.9:\n",
    "    fb = sentence_df.merge(\n",
    "        prod_rank_min[[\"product_name_cleaned\", \"subtopic_name_norm\", \"Topic_Score\"]],\n",
    "        left_on=[\"product_name_cleaned\", \"final_subtopic_name_norm\"],\n",
    "        right_on=[\"product_name_cleaned\", \"subtopic_name_norm\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    merged.loc[merged[\"Topic_Score\"].isna(), \"Topic_Score\"] = fb[\"Topic_Score\"].values\n",
    "    print(f\"After name-fallback coverage: {merged['Topic_Score'].notna().mean()*100:.1f}%\")\n",
    "\n",
    "# ---------------------------\n",
    "# Sentiment labeling (Pros/Cons)\n",
    "# ---------------------------\n",
    "sent_col_sent = \"sentiment_score\"\n",
    "merged[\"Category\"] = merged[sent_col_sent].apply(lambda x: \"Pro\" if x > 0 else (\"Con\" if x < 0 else \"Neutral\"))\n",
    "\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(merged[\"Category\"].value_counts())\n",
    "\n",
    "# ---------------------------\n",
    "# Extract top 5 subtopics per product for Pros and Cons separately\n",
    "# ---------------------------\n",
    "top_n_topics = 5\n",
    "\n",
    "# For Pros: higher average sentiment first (tie-break with prominence)\n",
    "pro_rank = (\n",
    "    prod_rank_min\n",
    "    .sort_values([\"product_name_cleaned\", sent_col_prod, prom_col],\n",
    "                 ascending=[True, False, False])\n",
    ")\n",
    "top_topics_pro = (\n",
    "    pro_rank.groupby(\"product_name_cleaned\")[\"subtopic_id_str\"]\n",
    "    .apply(lambda s: list(s.head(top_n_topics)))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# For Cons: lower average sentiment first (more negative), tie-break with prominence\n",
    "con_rank = (\n",
    "    prod_rank_min\n",
    "    .sort_values([\"product_name_cleaned\", sent_col_prod, prom_col],\n",
    "                 ascending=[True, True, False])\n",
    ")\n",
    "top_topics_con = (\n",
    "    con_rank.groupby(\"product_name_cleaned\")[\"subtopic_id_str\"]\n",
    "    .apply(lambda s: list(s.head(top_n_topics)))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "def _unique_preserve(seq):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in seq:\n",
    "        if x not in seen:\n",
    "            seen.add(x)\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n",
    "# ---------------------------\n",
    "# Extract representative sentences (using Pro/Con topic sets separately)\n",
    "# ---------------------------\n",
    "top_n_sentences = 5\n",
    "results = []\n",
    "\n",
    "for prod in product_df[\"product_name_cleaned\"].unique():\n",
    "    topics_pro = top_topics_pro.get(prod, [])\n",
    "    topics_con = top_topics_con.get(prod, [])\n",
    "    topics_combined = _unique_preserve(topics_pro + topics_con)  # keep original column name downstream\n",
    "\n",
    "    sub = merged[merged[\"product_name_cleaned\"] == prod].copy()\n",
    "\n",
    "    # Pros: from pro topic set, sort by Topic_Score desc then sentiment desc\n",
    "    pros = sub[\n",
    "        (sub[\"Category\"] == \"Pro\") &\n",
    "        (sub[\"final_subtopic_id_str\"].isin(topics_pro))\n",
    "    ].sort_values(by=[\"Topic_Score\", sent_col_sent], ascending=[False, False])\n",
    "\n",
    "    # Cons: from con topic set, sort by Topic_Score desc then sentiment asc (strong negatives)\n",
    "    cons = sub[\n",
    "        (sub[\"Category\"] == \"Con\") &\n",
    "        (sub[\"final_subtopic_id_str\"].isin(topics_con))\n",
    "    ].sort_values(by=[\"Topic_Score\", sent_col_sent], ascending=[False, True])\n",
    "\n",
    "    pros_list = pros[\"sentence_text\"].dropna().apply(clean_text).unique().tolist()[:top_n_sentences]\n",
    "    cons_list = cons[\"sentence_text\"].dropna().apply(clean_text).unique().tolist()[:top_n_sentences]\n",
    "\n",
    "    results.append({\n",
    "        \"product_name_cleaned\": prod,\n",
    "        \"Top_ProsCons_Topics\": \", \".join(topics_combined),  # keep existing column\n",
    "        \"Pros\": pros_list,\n",
    "        \"Cons\": cons_list,\n",
    "        \"Pros Summary\": \" \".join(pros_list),\n",
    "        \"Cons Summary\": \" \".join(cons_list),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nSample output:\")\n",
    "display(summary_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46192970-b991-46ce-8cca-f53d7e01605a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: task3_outputs\\Task3_Review_Summarization_Extractive_v2.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Export to Excel\n",
    "# ---------------------------\n",
    "out_dir = Path(\"task3_outputs\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "excel_path = out_dir / \"Task3_Review_Summarization_Extractive_v2.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as writer:\n",
    "    prod_rank_min.to_excel(writer, sheet_name=safe_sheet_name(\"Topic Ranking\"), index=False)\n",
    "    summary_df.to_excel(writer, sheet_name=safe_sheet_name(\"Pros_Cons_Summary\"), index=False)\n",
    "\n",
    "print(f\"✅ Saved: {excel_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b1a7e40-9b05-446c-be63-801d61a7450e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: C:\\Users\\zongy\\OneDrive\\Desktop\\SMU\\ISSS609 - Text Analytics and Applications\\Proj\\Task3_Review_Summarization_Generative_RuleBased_v2.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1) Imports & Config\n",
    "# ============================================\n",
    "import re, ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT_XLSX  = \"Task3_Review_Summarization_Extractive_v2.xlsx\"\n",
    "INPUT_SHEET = \"Pros_Cons_Summary\"\n",
    "OUTPUT_XLSX = \"Task3_Review_Summarization_Generative_RuleBased_v2.xlsx\"\n",
    "\n",
    "# ============================================\n",
    "# 2) Load Data (robust to list-or-string)\n",
    "# ============================================\n",
    "df = pd.read_excel(INPUT_XLSX, sheet_name=INPUT_SHEET)\n",
    "\n",
    "for col in [\"product_name_cleaned\",\"Pros\",\"Cons\",\"Pros Summary\",\"Cons Summary\"]:\n",
    "    if col not in df.columns: df[col] = \"\"\n",
    "\n",
    "def safe_list(x):\n",
    "    if isinstance(x, list): return x\n",
    "    if isinstance(x, str) and x.strip().startswith(\"[\"):\n",
    "        try:\n",
    "            v = ast.literal_eval(x)\n",
    "            return v if isinstance(v, list) else []\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def text_to_sentences(txt):\n",
    "    txt = str(txt or \"\").strip()\n",
    "    if not txt: return []\n",
    "    parts = re.split(r\"(?<=[.!?])\\s+\", txt)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def get_pros_list(row):\n",
    "    L = safe_list(row.get(\"Pros\", []))\n",
    "    return (L[:5] if L else text_to_sentences(row.get(\"Pros Summary\", \"\"))[:5])\n",
    "\n",
    "def get_cons_list(row):\n",
    "    L = safe_list(row.get(\"Cons\", []))\n",
    "    return (L[:5] if L else text_to_sentences(row.get(\"Cons Summary\", \"\"))[:5])\n",
    "\n",
    "# ============================================\n",
    "# 3) Cleaning & Normalization\n",
    "# ============================================\n",
    "MOJIBAKE = {\"â€™\":\"'\", \"â€œ\":'\"', \"â€\":'\"', \"â€“\":\"-\", \"â€”\":\"-\", \"â€¦\":\"...\", \"Â\":\"\"}\n",
    "\n",
    "DROP_PATTERNS = [\n",
    "    r\"\\bhopefully\\b\",\n",
    "    r\"\\bfind your\\b\",\n",
    "    r\"\\bworks for you\\b\",\n",
    "    r\"\\bhighly recommend(ed)?\\b\",\n",
    "    r\"\\bso worth it\\b\",\n",
    "    r\"^wow\\b\",\n",
    "]\n",
    "\n",
    "def normalize_sentence(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    for k,v in MOJIBAKE.items(): s = s.replace(k,v)\n",
    "    s = re.sub(r\"^\\s*\\[(?:pros?|cons?)\\]\\s*:?[\\s\\-]*\",\"\", s, flags=re.I)   # strip [Pros]/[Cons]\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    s = re.sub(r\"[!?]{2,}\", \"!\", s)\n",
    "    s = re.sub(r\"\\.\\s*\\.+\", \".\", s)\n",
    "    if s and s[0].isalpha(): s = s[0].upper() + s[1:]\n",
    "    if re.search(r\"[A-Za-z0-9)]$\", s): s += \".\"\n",
    "    return s\n",
    "\n",
    "def is_informative(s: str) -> bool:\n",
    "    t = re.sub(r\"[^a-z0-9]+\",\"\", s.lower())\n",
    "    if len(t) < 15: return False\n",
    "    if any(re.search(p, s.lower()) for p in DROP_PATTERNS): return False\n",
    "    return True\n",
    "\n",
    "def to_aggregate_voice(s: str) -> str:\n",
    "    s = re.sub(r\"\\bI\\b\", \"users\", s)\n",
    "    s = re.sub(r\"\\bI'm\\b\", \"users are\", s, flags=re.I)\n",
    "    s = re.sub(r\"\\bmy\\b\", \"their\", s, flags=re.I)\n",
    "    s = re.sub(r\"\\bme\\b\", \"them\", s, flags=re.I)\n",
    "    s = re.sub(r\"\\bwe\\b\", \"users\", s, flags=re.I)\n",
    "    s = re.sub(r\"\\bours\\b\", \"their\", s, flags=re.I)\n",
    "    # soften superlatives\n",
    "    s = re.sub(r\"\\bmy all[- ]time favorite\\b\", \"a customer favorite\", s, flags=re.I)\n",
    "    s = re.sub(r\"\\bbest retinol product\\b\", \"one of the best retinol products\", s, flags=re.I)\n",
    "    return s\n",
    "\n",
    "def prepare_sentences(raw_list):\n",
    "    out = []\n",
    "    for s in raw_list:\n",
    "        s = normalize_sentence(s)\n",
    "        if not s or not is_informative(s): continue\n",
    "        s = to_aggregate_voice(s)\n",
    "        out.append(s)\n",
    "    # de-duplicate (case/punct insensitive)\n",
    "    seen, uniq = set(), []\n",
    "    for s in out:\n",
    "        key = re.sub(r\"[^a-z0-9]+\",\"\", s.lower())\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            uniq.append(s)\n",
    "    return uniq[:5]\n",
    "\n",
    "# ============================================\n",
    "# 4) Lightweight Theme Detection\n",
    "# ============================================\n",
    "# We flag coarse themes to craft natural prose.\n",
    "PROS_THEMES = {\n",
    "    \"effectiveness\": [r\"improv\", r\"results?\", r\"worked\", r\"difference\", r\"clear\", r\"texture\", r\"softer\", r\"glow\", r\"acne\", r\"breakout\"],\n",
    "    \"quick_results\": [r\"after (a few|two|couple) (uses|weeks|days)\", r\"quick\", r\"fast\", r\"within\"],\n",
    "    \"favorite_best\": [r\"favorite\", r\"one of the best\", r\"best\", r\"holy grail\"],\n",
    "    \"feel_texture\": [r\"light(weight)?\", r\"absorb\", r\"non[- ]greasy\", r\"smooth\", r\"gentle\"],\n",
    "}\n",
    "\n",
    "CONS_THEMES = {\n",
    "    \"dry_peel\": [r\"dry\", r\"peel\", r\"flak\", r\"dehydrat\", r\"tight\"],\n",
    "    \"red_irritate\": [r\"red\", r\"irritat\", r\"sensitive\", r\"burn\"],\n",
    "    \"breakouts\": [r\"pimple\", r\"breakout\", r\"acne\", r\"bumpy\"],\n",
    "    \"strong_react\": [r\"worst\", r\"severe\", r\"react\", r\"rash\"],\n",
    "}\n",
    "\n",
    "def flag_themes(sentences, theme_dict):\n",
    "    flags = {k: False for k in theme_dict}\n",
    "    lower = \" \".join(sentences).lower()\n",
    "    for k, pats in theme_dict.items():\n",
    "        flags[k] = any(re.search(p, lower) for p in pats)\n",
    "    return flags\n",
    "\n",
    "# ============================================\n",
    "# 5) Natural Renderers (phrase banks)\n",
    "# ============================================\n",
    "def render_pros(sentences):\n",
    "    if not sentences: return \"\"\n",
    "    flags = flag_themes(sentences, PROS_THEMES)\n",
    "    parts = []\n",
    "\n",
    "    if flags[\"favorite_best\"]:\n",
    "        parts.append(\"many consider it a customer favorite and among the better over-the-counter options\")\n",
    "    if flags[\"effectiveness\"]:\n",
    "        parts.append(\"it delivers noticeable improvements to skin texture and clarity\")\n",
    "    if flags[\"quick_results\"]:\n",
    "        parts.append(\"with some seeing results within a few uses\")\n",
    "    if flags[\"feel_texture\"]:\n",
    "        parts.append(\"and the texture feels lightweight and absorbs well\")\n",
    "\n",
    "    if not parts:\n",
    "        # fallback to first two sentences paraphrased lightly\n",
    "        s1 = re.sub(r\"\\.\\s*$\",\"\", sentences[0])\n",
    "        s2 = re.sub(r\"\\.\\s*$\",\"\", sentences[1]) if len(sentences)>1 else \"\"\n",
    "        out = f\"Customers appreciated that {s1[0].lower()+s1[1:]}.\"\n",
    "        if s2:\n",
    "            out += f\" In addition, {s2[0].lower()+s2[1:]}.\"\n",
    "        return out\n",
    "\n",
    "    # build up to two sentences\n",
    "    sent1 = \"Customers appreciated that \" + parts[0] + \".\"\n",
    "    extra = parts[1:]\n",
    "    if extra:\n",
    "        # join 1–2 extra parts naturally\n",
    "        if len(extra) == 1:\n",
    "            sent2 = extra[0].capitalize() + \".\"\n",
    "        else:\n",
    "            sent2 = (extra[0].capitalize() + \", \" + extra[1] + \".\")\n",
    "        return f\"{sent1} {sent2}\"\n",
    "    return sent1\n",
    "\n",
    "NEG_OPENERS = [\n",
    "    \"Some users experienced\",\n",
    "    \"Several reviewers reported\",\n",
    "    \"A number of customers mentioned\",\n",
    "]\n",
    "\n",
    "def render_cons(sentences):\n",
    "    if not sentences: return \"\"\n",
    "    flags = flag_themes(sentences, CONS_THEMES)\n",
    "    items = []\n",
    "\n",
    "    if flags[\"dry_peel\"]:\n",
    "        items.append(\"dryness or peeling\")\n",
    "    if flags[\"red_irritate\"]:\n",
    "        items.append(\"redness or irritation\")\n",
    "    if flags[\"breakouts\"]:\n",
    "        items.append(\"next-day breakouts or bumpy skin\")\n",
    "    if flags[\"strong_react\"]:\n",
    "        items.append(\"strong adverse reactions in a few cases\")\n",
    "\n",
    "    opener = NEG_OPENERS[min(len(sentences)-1, len(NEG_OPENERS)-1)]\n",
    "    if not items:\n",
    "        # fallback to first two negatives\n",
    "        s1 = re.sub(r\"\\.\\s*$\",\"\", sentences[0])\n",
    "        s2 = re.sub(r\"\\.\\s*$\",\"\", sentences[1]) if len(sentences)>1 else \"\"\n",
    "        if s2:\n",
    "            return f\"{opener} {s1[0].lower()+s1[1:]}, and {s2[0].lower()+s2[1:]}.\"\n",
    "        else:\n",
    "            return f\"{opener} {s1[0].lower()+s1[1:]}.\"\n",
    "    else:\n",
    "        if len(items) == 1:\n",
    "            return f\"{opener} {items[0]}.\"\n",
    "        if len(items) == 2:\n",
    "            return f\"{opener} {items[0]} and {items[1]}.\"\n",
    "        # 3+ items\n",
    "        return f\"{opener} {', '.join(items[:-1])}, and {items[-1]}.\"\n",
    "\n",
    "def strip_lead_for_overall(text, lead=\"Customers appreciated that \"):\n",
    "    t = text.strip()\n",
    "    if t.lower().startswith(lead.lower()):\n",
    "        t = t[len(lead):].strip()\n",
    "        if t and t[0].isalpha(): t = t[0].lower()+t[1:]\n",
    "    return t\n",
    "\n",
    "def tone_from_lengths(pros_txt, cons_txt):\n",
    "    lp, lc = len(pros_txt.split()), len(cons_txt.split())\n",
    "    total = max(lp+lc, 1)\n",
    "    r = lp/total\n",
    "    if r >= 0.65: return \"positive\"\n",
    "    if r <= 0.35: return \"negative\"\n",
    "    return \"balanced\"\n",
    "\n",
    "def render_overall(pros_par, cons_par, tone):\n",
    "    pros_body = strip_lead_for_overall(pros_par) if pros_par else \"\"\n",
    "    cons_body = cons_par.strip()\n",
    "    if pros_body and cons_body:\n",
    "        if tone == \"positive\":\n",
    "            return f\"Overall, users found that {pros_body} However, {cons_body[0].lower()+cons_body[1:]}\"\n",
    "        elif tone == \"negative\":\n",
    "            return f\"Overall, users found that {cons_body} On the plus side, {pros_body[0].lower()+pros_body[1:]}\"\n",
    "        else:\n",
    "            return f\"Overall, users found that {pros_body} However, {cons_body[0].lower()+cons_body[1:]}\"\n",
    "    return pros_par or cons_par\n",
    "\n",
    "# ============================================\n",
    "# 6) Generate & Export\n",
    "# ============================================\n",
    "pros_out, cons_out, overall_out, tones = [], [], [], []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    pros_raw = get_pros_list(row)\n",
    "    cons_raw = get_cons_list(row)\n",
    "\n",
    "    pros_clean = prepare_sentences(pros_raw)\n",
    "    cons_clean = prepare_sentences(cons_raw)\n",
    "\n",
    "    pros_par = render_pros(pros_clean)\n",
    "    cons_par = render_cons(cons_clean)\n",
    "    tone = tone_from_lengths(\" \".join(pros_clean), \" \".join(cons_clean))\n",
    "    overall_par = render_overall(pros_par, cons_par, tone)\n",
    "\n",
    "    pros_out.append(pros_par)\n",
    "    cons_out.append(cons_par)\n",
    "    overall_out.append(overall_par)\n",
    "    tones.append(tone)\n",
    "\n",
    "df[\"Pros_Generated\"] = pros_out\n",
    "df[\"Cons_Generated\"] = cons_out\n",
    "df[\"Overall_Generated\"] = overall_out\n",
    "df[\"Overall_Tone\"] = tones\n",
    "\n",
    "cols_keep = [c for c in [\n",
    "    \"product_name_cleaned\",\n",
    "    \"Pros\",\"Cons\",\"Pros Summary\",\"Cons Summary\",\n",
    "    \"Pros_Generated\",\"Cons_Generated\",\"Overall_Generated\",\"Overall_Tone\"\n",
    "] if c in df.columns]\n",
    "\n",
    "out_path = Path(OUTPUT_XLSX)\n",
    "with pd.ExcelWriter(out_path, engine=\"openpyxl\") as w:\n",
    "    df[cols_keep].to_excel(w, index=False, sheet_name=\"Generated_Summaries\")\n",
    "\n",
    "print(f\"✅ Saved: {out_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c492df1d-e513-4c81-83c8-b571dd81a67f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
